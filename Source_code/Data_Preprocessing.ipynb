{"cells":[{"cell_type":"code","execution_count":15,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1747328477599,"user":{"displayName":"Prateek Hiremath","userId":"08043554161074675514"},"user_tz":-330},"id":"2m9VW6LI2Wcf"},"outputs":[],"source":["import pandas as pd\n","import re\n","import nltk\n","import spacy\n","import matplotlib.pyplot as plt\n","from nltk.tokenize import sent_tokenize, word_tokenize\n","from nltk import pos_tag\n","from nltk.corpus import wordnet as wn, stopwords\n","from nltk.stem.wordnet import WordNetLemmatizer\n","from collections import defaultdict, Counter\n","import subprocess"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":627,"status":"ok","timestamp":1747328479873,"user":{"displayName":"Prateek Hiremath","userId":"08043554161074675514"},"user_tz":-330},"id":"FPmCzQ9U3IjV","outputId":"4b6e989c-98bf-4091-bac9-07cfb816e2ae"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n","[nltk_data]   Package punkt_tab is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n","[nltk_data]   Package omw-1.4 is already up-to-date!\n","[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"]}],"source":["# Download NLTK resources\n","nltk.download('punkt_tab')\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","nltk.download('omw-1.4')\n","nltk.download('averaged_perceptron_tagger_eng')\n","\n","# Load spaCy model\n","try:\n","    nlp = spacy.load(\"en_core_web_sm\")\n","except OSError:\n","    subprocess.run([\"python\", \"-m\", \"spacy\", \"download\", \"en_core_web_sm\"], check=True)\n","    nlp = spacy.load(\"en_core_web_sm\")\n"]},{"cell_type":"code","execution_count":17,"metadata":{"executionInfo":{"elapsed":618,"status":"ok","timestamp":1747328483458,"user":{"displayName":"Prateek Hiremath","userId":"08043554161074675514"},"user_tz":-330},"id":"DCNQ1EgH3NJg"},"outputs":[],"source":["# =============================\n","# STEP 1: Load \u0026 Clean Datasets\n","# =============================\n","\n","# Load Enron and SMS datasets\n","enron_df = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/enron emails.csv\", encoding='ISO-8859-1')\n","sms_df = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/mail_data.csv\")\n","\n","# Clean Enron\n","enron_df = enron_df[['Message', 'Category']].rename(columns={'Message': 'message', 'Category': 'label'})\n","# Clean SMS\n","sms_df = sms_df.rename(columns={'Message': 'message', 'Category': 'label'})\n","\n","# Encode labels\n","combined_df = pd.concat([enron_df, sms_df], ignore_index=True)\n","combined_df['label'] = combined_df['label'].map({'ham': 0, 'spam': 1})\n","combined_df.dropna(subset=['message', 'label'], inplace=True)\n","combined_df.drop_duplicates(subset=['message'], inplace=True)\n","combined_df.reset_index(drop=True, inplace=True)"]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2917,"status":"ok","timestamp":1747328489401,"user":{"displayName":"Prateek Hiremath","userId":"08043554161074675514"},"user_tz":-330},"id":"FRHdj8it3UJv","outputId":"1af3b730-ff8f-4470-cd21-68640d908dbe"},"outputs":[{"name":"stdout","output_type":"stream","text":["Total samples: 35595\n","Label distribution:\n"," label\n","0.0    20413\n","1.0    15182\n","Name: count, dtype: int64\n","Text length stats:\n"," count    35595.000000\n","mean      1171.920438\n","std       1809.583574\n","min          2.000000\n","25%        225.000000\n","50%        577.000000\n","75%       1375.000000\n","max      32453.000000\n","Name: text_length, dtype: float64\n","Top 20 frequent words:\n","[('.', 412805), ('-', 365855), (',', 298987), ('the', 220446), ('to', 170955), ('/', 157280), (':', 153732), ('and', 124095), ('of', 112471), ('a', 90473), ('in', 79526), ('you', 73681), ('for', 70187), ('is', 59098), ('this', 55052), ('_', 54237), ('i', 53573), (\"'\", 50127), ('on', 48056), (')', 45804)]\n"]}],"source":["# =============================\n","# STEP 2: Exploratory Data Analysis\n","# =============================\n","print(\"Total samples:\", len(combined_df))\n","print(\"Label distribution:\\n\", combined_df['label'].value_counts())\n","\n","combined_df['label'].value_counts().plot(kind='bar')\n","plt.title(\"Spam vs Ham Distribution\")\n","plt.xlabel(\"Label (0 = Ham, 1 = Spam)\")\n","plt.ylabel(\"Count\")\n","plt.tight_layout()\n","plt.savefig(\"/content/drive/MyDrive/Colab Notebooks/eda_label_distribution.png\")\n","plt.close()\n","\n","combined_df['text_length'] = combined_df['message'].apply(len)\n","print(\"Text length stats:\\n\", combined_df['text_length'].describe())\n","\n","combined_df[combined_df['text_length'] \u003c 5000]['text_length'].hist(bins=50)\n","plt.title(\"Message Length Distribution (\u003c5000 chars)\")\n","plt.xlabel(\"Length\")\n","plt.ylabel(\"Frequency\")\n","plt.tight_layout()\n","plt.savefig(\"/content/drive/MyDrive/Colab Notebooks/eda_text_length.png\")\n","plt.close()\n","\n","print(\"Top 20 frequent words:\")\n","print(Counter(\" \".join(combined_df['message'].str.lower()).split()).most_common(20))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"FusTK5fe3YRf"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","✅ Done. Saved to 'processed_emails_combined.csv'\n"]}],"source":["# =============================\n","# STEP 3: Text Preprocessing\n","# =============================\n","lemmatizer = WordNetLemmatizer()\n","stop_words = set(stopwords.words('english'))\n","tag_map = defaultdict(lambda: wn.NOUN)\n","tag_map['J'] = wn.ADJ\n","tag_map['V'] = wn.VERB\n","tag_map['R'] = wn.ADV\n","\n","def process_sentence(sentence):\n","    nouns = []\n","    base_words = []\n","    final_words = []\n","    words_2 = word_tokenize(sentence)\n","    sentence = re.sub(r'[^ \\w\\s]', '', sentence)\n","    sentence = re.sub(r'_', ' ', sentence)\n","    words = word_tokenize(sentence)\n","    pos_tagged_words = pos_tag(words)\n","    for token, tag in pos_tagged_words:\n","        base_words.append(lemmatizer.lemmatize(token, tag_map[tag[0]]))\n","    for word in base_words:\n","        if word not in stop_words:\n","            final_words.append(word)\n","    sent = \" \".join(final_words)\n","    for token, tag in pos_tag(words_2):\n","        if tag == 'NN' and len(token) \u003e 1:\n","            nouns.append(token)\n","    return sent, nouns\n","\n","def clean(email):\n","    email = email.lower()\n","    sentences = sent_tokenize(email)\n","    total_nouns = []\n","    string = \"\"\n","    for sent in sentences:\n","        sentence, nouns = process_sentence(sent)\n","        string += \" \" + sentence\n","        total_nouns += nouns\n","    return string.strip(), total_nouns\n","\n","def ents(text):\n","    doc = nlp(text)\n","    expls = dict()\n","    if doc.ents:\n","        for ent in doc.ents:\n","            label = ent.label_\n","            word = ent.text\n","            if label in expls:\n","                expls[label].append(word)\n","            else:\n","                expls[label] = [word]\n","        return expls\n","    return 'no'\n","\n","# Apply preprocessing\n","combined_df['cleaned_text'], combined_df['nouns'] = zip(*combined_df['message'].apply(clean))\n","combined_df['entities'] = combined_df['message'].apply(ents)\n","\n","# Save\n","combined_df.drop(columns=['text_length'], inplace=True)\n","combined_df.to_csv(\"/content/drive/MyDrive/Colab Notebooks/processed_emails_combined.csv\", index=False)\n","print(\"\\n✅ Done. Saved to 'processed_emails_combined.csv'\")\n"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyOwrc6FiuNH26z2rDHh7eqe","gpuType":"T4","mount_file_id":"17n0SwJeHcN0hpfJyv8NCqXPShX4hG51D","name":"","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}